# Day 8: RL Framework Design Space

Welcome to Day 8 of the GPU Challenge!

> Let’s explore the design space of RL frameworks for LLM post-training and the engineering challenges involved in scaling them.

As discussed at [**Day 7**](./day-7.md), the world of large language models (LLMs) is buzzing with the promise of reinforcement learning (RL). Breakthroughs like DeepSeek's R1 and OpenAI's o1 have shown that RL can not only align models with human values but also significantly enhance their reasoning abilities. As a result, RL post-training is no longer an afterthought but a critical, demanding phase in LLM development.

This shift, however, brings a new set of engineering challenges, especially when it comes to scaling the underlying frameworks.

#### Two "Dependent" Tasks

At the heart of RL post-training lies a fundamental duality. The process is split into two distinct yet intertwined components: **rollout** and **training**.

*   **Rollout (Inference):** This is where the model, like a student solving a problem, generates multiple responses or "rollouts" for a given prompt. This phase is intensely memory-bound due to the need to store Key-Value (KV) caches for each generated token, a problem that grows with longer context windows and complex chain-of-thought reasoning.
*   **Training:** Here, the model's parameters are updated based on the feedback from the rollouts. This phase is computationally intensive, demanding raw processing power to perform backpropagation and update the model's trillions of parameters.

The challenge intensifies because on-policy RL algorithms—widely considered the most effective—require that training data be generated by the *current* version of the model. This creates a sequential dependency: you must perform rollouts, then train on that data, then repeat. This lockstep relationship can lead to significant resource underutilization, as one expensive component sits idle while the other is active.

#### Architectural Showdown: Collocated vs. Separated

To manage this complex interplay, two primary architectural patterns have emerged:

*   **Task-Collocated Architecture:** In this setup, both rollout and training tasks run on the same set of GPUs in a time-multiplexed fashion. It's a simpler approach where data transfer is straightforward. However, this simplicity comes at a steep cost to efficiency. The sequential nature of on-policy RL means your expensive hardware is often idle.

    ```
    +-------------------------------------------------+
    | GPU Cluster                                     |
    |                                                 |
    |  +-----------+     +----------+     +-----------+
    |  | Rollout   | --> | Training | --> | Rollout   |
    |  +-----------+     +----------+     +-----------+
    |                                                 |
    +-------------------------------------------------+
    ```

*   **Task-Separated Architecture:** This more advanced architecture decouples the rollout and training tasks, allowing them to run on different, potentially heterogeneous, sets of devices. This spatial multiplexing enables far more efficient use of resources. For instance, inference-optimized GPUs can be dedicated to rollouts, while compute-heavy GPUs focus on training. This approach, while more efficient, introduces new complexities in data management and synchronization.

    ```
    +--------------------------+      +-------------------------+
    | Rollout Cluster          |      | Training Cluster        |
    | (Inference-Optimized)    |      | (Compute-Optimized)     |
    |                          |      |                         |
    |  +-------------------+   |      |   +-----------------+   |
    |  | Generates         |   |      |   | Updates         |   |
    |  | experience data   | ------>  |   | model parameters|   |
    |  +-------------------+   |      |   +-----------------+   |
    |                          |      |                         |
    +--------------------------+      +-------------------------+
    ```

#### Navigating the Trade-offs

As an engineer designing an RL framework, you're faced with a series of critical decisions:

*   **Challenge 1: Resource Management & Synchronization:** How do you efficiently manage resources when rollout is memory-bound and training is compute-bound? How do you synchronize model parameters between these distinct phases without creating bottlenecks or stalls? Separated systems promise better utilization but require sophisticated, low-latency mechanisms for data and parameter transfer.

*   **Challenge 2: The Dataflow Maze:** The LLM ecosystem is a vibrant but fragmented landscape of training frameworks (Megatron-LM, DeepSpeed, FSDP) and inference engines (vLLM, SGLang). Each combination presents a unique integration challenge. A modular approach is more future-proof but requires designing complex yet clean interfaces.

*   **Challenge 3: The Asynchronicity & Granularity:** Rollouts can have wildly different completion times, especially with AI agents generating complex outputs. Waiting for an entire batch to finish creates "bubbles" of idle resources. A dynamic, asynchronous system that processes data at a finer granularity (per-sample or micro-batch) is more efficient but requires a robust data management system, potentially involving centralized queues or message brokers.

---

### The Long-Tail Problem and Solution

Let's make this concrete. Take [Slime](https://github.com/THUDM/slime/) for case study.

The rollout phase often dominates the overall training time in on-policy RLHF. Response generation time is highly variable, and a few unusually long sequences can stall the entire process. As a result, many GPUs sit idle, waiting for the slowest rollouts to complete, results in cluster underutilization and slowing down the training loop.

A practical solution to this issue is implementing a **Partial Rollout** strategy, also known as **Early Stopping**. This system-level optimization reduces idle time and boosts throughput.

How Partial Rollout Works:

**1. Over-Sample Proactively**

Instead of generating exactly the number of rollouts needed per training step, the system launches more than necessary. For example, if the trainer needs 8 samples (`rollout_batch_size = 8`), the inference engine might speculatively start 64 rollouts (`over_sampling_batch_size = 64`).

**2. Terminate Once Enough is Collected**

As soon as 8 valid rollouts are completed, the system sends an early-stop signal to the inference engine (e.g., SGLang), aborting the rest of the ongoing generations.

**3. Recycle Partial Trajectories**

Importantly, the partially completed rollouts aren’t wasted. They are stored in a buffer and resumed in the next iteration, continuing from where they left off.

This mechanism is easy to integrate into existing RLHF pipelines without too much modifications. It’s a great example of how thoughtful engineering trick can lead to faster training and more efficient resource use.

---

### Guiding Principles for an Unknown Future

The challenges are clear, but how do we build systems that last? The LLM space is evolving at a breakneck pace. Any framework built today must be able to adapt to algorithms, models, and hardware we haven't even conceived of yet. Thinking from first principles, here are some core tenets to guide the design:

*   **Principle 1: Keep Interfaces Simple.** The more complex your inter-service APIs, the harder it is to swap out components or add new capabilities. Simple, well-defined, and stable interfaces (like a `put`/`get` semantic for data exchange) are your best defense against future complexity. Besides, It's easier to leverage **from** or **to** each other's work.
*   **Principle 2: Embrace Heterogeneity.** Don't assume uniform compute. Your framework should gracefully handle a mix of high-memory GPUs for inference, compute-dense accelerators for training, and even CPUs for reward models. Design abstractions that make hardware differences an implementation detail, not a user-facing problem.
*   **Principle 3: Prepare for Failure.** Distributed systems fail. It's a fact of life. A four-hour training run should not restart from scratch because one node hiccuped. Build in robust checkpointing, graceful degradation, and automatic recovery from the start.
*   **Principle 4: Make Everything Observable.** In a complex distributed system, visibility is non-negotiable. You need to know more than "training is 47% complete." You need to see queue depths, GPU utilization, network latencies, and reward computation bottlenecks in real-time.


### The Bottom Line

There is no single "right" architecture for RL training. The best choice depends on your team size, hardware budget, performance targets, and tolerance for operational complexity.

However, one thing is certain: the simple, monolithic approach does not scale for modern RL workloads. Whether you go with a framework like Ray, manage your own cluster, or find a hybrid approach, you must embrace the complexity of distributed systems.

***

*What's your take on RL framework design? Have you hit similar challenges or found different solutions? I'd love to hear about your experiences. Feel free to email your thoughts to me!*


### Suggested Projects

* [VeRL](https://github.com/volcengine/verl)

* [Slime](https://github.com/THUDM/slime)

* [LlamaRL](https://arxiv.org/pdf/2505.24034)

* [AReaL](https://github.com/inclusionAI/AReaL)

* [Next RL Framework](Suggest one or fill your own!)


### Suggested Readings

* [A Great Slide of Introducing VeRL by Yuxuan Tong](https://tongyx361.github.io/blogs/posts/verl-intro/)

* [An Optimization Journey of Memory Management in VeRL and SGLang by Biao He](https://hebiao064.github.io/rl-memory-management)